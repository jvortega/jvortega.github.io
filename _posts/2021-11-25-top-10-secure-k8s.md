---
layout: post
title: 10 Ways to Secure Kubernetes
categories: Kubernetes
---

## Overview (h2)

A few concepts before diving into securing a Kubernetes cluster.

Kubernetes is a container platform that orchestrates computing, networking and storage of immutable infrastructure workloads, where containers are package code, runtime, system tools, system libraries, and configs altogether.

## Cluster Node Hardening (h2)

Hardening the underlying Operating System that the Kubernetes cluster nodes will be running on is fundamental to ensure a secure deployment.

Once the nodes are hardened, the runtime for the containers whether it be Docker or Containerd also need to be hardened and finally the orchestration layer; Kubernetes.

## Authentication (h2)

Interactions with Kubernetes should and are mostly done by communicating with the control plane, specifically the kube-apiserver component of the control plane.

Requests to the kube-apiserver are accepted or rejected based on 3 gates that the request must go through.
Authentication, Authorization and finally Admission Controls.

By default Kubernetes authentication is set to anonymous, so this setting needs to be set to false.

```
$ kube-apiserver       \
    <... other flags>  \
    —anonymous-auth=false
```

Connections to the kube-apiserver are also unencrypted by default, so this setting must also be changed to ensure a secure connection.

```
$ kube-apiserver      \
    <... other flags> \
    —insecure-port=0
```

Additionally considerations must be taken to protect access to the kube-apiserver at the network layer, whether the control plane is on a private network or in the public cloud.

Lastly connections to the kube-apiserver from a user’s workstation are governed by a Kubeconfig file under the user’s profile.

## Authorization (h2)

Out-of-the-box Kubernetes is configured to authorize all requests to the kube-apiserver.

```$ kube-apiserver      \
    <... other flags> \
    —authorization-mode=RBAC
```

Also by default, there is a service account that is automatically mounted in all containers in Kubernetes. Without RBAC enabled in the cluster, this account has extensive permissions on the cluster, so ensure this is not the case.

```$ kubectl patch service account default -p "automountServiceAccountToken: false"
```

## Admission Controls (h2)

The last of the three gates, more than a gate is actually a proxy capable of not only creating, deleting or updating an object but also modifying the request 

Ensure that admission control denies privilege escalation.

```$ kube-apiserver       \
    <...other flags> \
    —admission-control=...,DenyEscalatingExec
```

Ensure that admission control is set to always pull images, instead of the ones Kubernetes caches. This will force Kubernetes to always present credentials before pulling images to run

```$ kube-apiserver      \
    <... other flags> \
    —admission-control=...,AlwaysPullImages
```

## Impersonation (h2)

This is a feature of Kubernetes, that allows one user to impersonate any other user within Kubernetes. This obviously has security implications if not well understood or properly setup to prevent abuse.

Ensure limitations are in place to who can impersonate users.

```apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: limited-impersonator
rules:
# Can impersonate the group "developers"
- apiGroups: [""]
  resources: ["groups"]
  verbs: ["impersonate"]
  resourceNames: [“developers"]
```

## Policies (h2)

Kubernetes decouples policy decision making from the kube-server by means of utilizing admission controller webhooks that intercept admission requests before they end up as objects persisting in Kubernetes.

There are a wide range of policies which could be enforced via Gatekeeper which is an admission control webhook for Open Policy Agent (OPA) but they are beyond the scope of this article.

Here are some policy examples:

1. Prevent privilege escalation
2. Prevent privileged runtime
3. Prevent access to the host filesystem
4. Restrict to certain allowed ports
5. Restrict to certain allowed images

## Network Policies (h2)

Networking is a critical component of Kubernetes, as it enables communications between system components as well as serving as a platform to run distributed systems and share machines between application without allocating ports.

Unlike a traditional network with switches and routers requiring the mapping of ports between containers and hosts, Kubernetes leverages a flat network design which reduces latency and complexity but which again has security implications. 

Any pod can communicate with any other pod without any restrictions by default.

In order to prevent this default behavior, Kubernetes exposes the Container Network Interface (CNI), so that third-party plugins can shape communications by inserting network interfaces in containers, assign IP addresses to interfaces and delete interfaces when containers are deleted.

Amongst the most famous third-party CNI plugins are Calico, Weave, Cilium and Flannel.

The type of policies enforced would dependent on the size of workloads on the cluster and its use. For example, a multi-tiered application would have policies restricting traffic inbound to any ingress for the cluster, the policies restricting traffic from the ingress to the front-end micro services and then from those front-end to back-end micro services. Outbound connections to any external systems would also have to be defined. Finally a global policy would explicitly deny any traffic not previous defined in network policies.

## Logging (h2)

When an application running on a traditional server or virtual machine dies or crashes, logs are still available on the machine. In Kubernetes when a pod is deleted, evicted, or crashes, the system cleans up after itself and automatically deletes the logs.

It is therefore critical to send relevant logs to a centralized logging or Security Information and Event Management system (SIEM).

The logs required are:

1. Node logs
2. Runtime logs (Docker/Containerd)
3. Kubernetes logs (kube-scheduler, kube-apiserver, controller and audit)
4. Application logs (ingress controller, microservices)

The node logs are generally in place as part of an organization existing logging strategy by picking up logs from the /var/log directory.

Whether the runtime for the Kubernetes is Docker, Containerd or CRI-O, these logs need to be sent or picked up by a logging agent on each node of the cluster.

Kubernetes cluster logs including all of its system components  also needs to be sent to a centralized location. This could be easily configured in many of the cloud flavors of k8s; for example AWS EKS where cluster logging for these types would be to be enabled or Azure AKS diagnostic logging for these types would also need to be enabled.
A bare-metal implementation might require sending the logs to the nodes /var/log directory for them to be picked up by an agent running on the node or an agent running as a side-car pickup these logs .

Application logs make an important part of logging the Kubernetes ecosystem, from the microservices running in the cluster to ingress controllers used to expose those services outside the cluster.

## Restricting access to the API (h2)

Even though the kube-server or API Server as its commonly known, has three gates as described earlier before a command is executed, a vulnerability or substandard implementation would allow an attacker to take control of the cluster specially if the API Server is facing the Internet.

Access control lists (ACL) or whitelisting traffic to the API Server from trusted sources is fundamental in offering a defense in depth approach to securing a Kubernetes cluster.

## Vulnerabilities (h2)

Now that we have gone through 9 specific areas of security when it comes down to Kubernetes, we have a much better understanding of the components that need to be under our Vulnerability Management program to ensure that vulnerabilities are addressed in a prompt and consistent manner.

1. Node vulnerabilities
2. Runtime vulnerabilities (Docker, Containerd, CRI-O)
3. Kubernetes vulnerabilities
4. Application vulnerabilities (container apps, tools, etc.)

Nodes vulnerabilities should be handled like you would any other server vulnerability, with the master nodes taking priority over the worker nodes.

Whatever the runtime of choice for the Kubernetes cluster needs to be managed as well, with the criticality dependent on threat vector amongst other variables.

Kubernetes vulnerabilities can be addressed in minor or mayor releases. Cloud vendors often release and push minor versions weekly and often up to a nightly basis, expecting customers to reboot nodes when necessary for updates to take effect. Major upgrades generally involve more hand holding but can be executed without downtime, by performing a rolling upgrade as workloads are moved away from the node that is being upgraded.

When it comes to the containers running on the Kubernetes cluster, wether it be microservices or admin tools, scans should be ran after the build process and before the images are pushed to the registry to guarantee they are vulnerability free. Continuous scans of the registry and mapping image vulnerabilities to running containers allow for a cycle of rebuilding a new image that is patched, delete the running container and replace it with one running from the newer image.